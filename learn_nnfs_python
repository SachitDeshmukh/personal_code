# -*- coding: utf-8 -*-
"""
Created on Mon Dec 30 11:31:11 2024

@author: Sachit Deshmukh
"""
from IPython import get_ipython

#==============================================================================
# OPTIMIZING THE INPUT LAYERS
#==============================================================================

# storing the set of input as list

inputs = [1, 2, 3, 0.5]

# storing the sets of wieghts between neuron connections as list of lists

weights = [[0.1, -0.2, 1.2, -1.5],
           [-0.5, 1.0, 0.4, 0.5],
           [2.0, 1.7, 0.6, -1.8]]

# storing the biases of neurons as list

biases = [2, 3, 0.5]

"""
merge_1 = zip(weights, biases)
print(list(merge_1))

[([0.1, -0.2, 1.2, -1.5], 2),
 ([-0.5, 1.0, 0.4, 0.5], 3),
 ([2.0, -1.7, 0.6, -1.8], 0.5)]

merge_2 = zip(inputs, weights[0])
print(list(merge_2))

print(weights[0])
"""

layer_outputs = []
for neuron_weights, neuron_bias in zip(weights, biases):
    neuron_output = 0 # created three output variables
    for n_input, weight in zip(inputs, neuron_weights):
        neuron_output += n_input*weight # calculated input*weight part of output
    neuron_output += neuron_bias # added biases to the outputs
    layer_outputs.append(neuron_output) # storing all outputs as list
    
print(layer_outputs) # output: [4.55, 5.95, 6.8]

#==============================================================================

"""
inputs = [1, 2, 3, 0.5]

weights = [[0.1, -0.2, 1.2, -1.5],
           [-0.5, 1.0, 0.4, 0.5],
           [2.0, 1.7, 0.6, -1.8]]

biases = [2, 3, 0.5]

trial_output = 0
for i in range(len(inputs)):
    value = inputs[i]*weights[0][i]
    trial_output += value
trial_output += biases[0]

print(trial_output)

print(len(weights))

trial_output_2 = []
for y in range(len(weights)):
    calc_output = 0
    for i in range(len(inputs)):
        calc_value = inputs[i]*weights[y][i]
        calc_output += calc_value
    calc_output += biases[y]
    trial_output_2.append(calc_output)    

print(trial_output_2)
"""
#==============================================================================
# DOT PRODUCTS
#==============================================================================

# trying out ways of optimizing the calculations

import numpy as np

inputs_t1 = [1, 2, 3, 0.5]
weights_t1 = [0.1, -0.2, 1.2, -1.5]
bias_t1 = 2

value_t1 = np.dot(inputs_t1, weights_t1)
print(value_t1) # output: 2.55
output_t1 = value_t1 + bias_t1
print(output_t1) # output: 4.55

# taking all the weights into the calculation

inputs = [1, 2, 3, 0.5]

weights = [[0.1, -0.2, 1.2, -1.5],
           [-0.5, 1.0, 0.4, 0.5],
           [2.0, 1.7, 0.6, -1.8]]

biases = [2, 3, 0.5]

layer_outputs_2 = np.dot(weights, inputs) + biases
print(layer_outputs_2) # output: [4.55, 5.95, 6.8] as expected

layer_outputs_2 = np.dot(inputs, weights) + biases
print(layer_outputs_2) # ValueError: shapes (4,) and (3,4) not aligned

"""
This errors shows as the matrix of inputs and weights does not match the shape
and hence is not able to perform the dot product function.

What we need to do is align the number of columns in the first matrix with the 
number of rows in the second matrix.

Since inputs is a vector, when added as the second matrix for dot product,
the function utilized the 4 values of the list as 1 individual column; and we
obtain another ventor with 3 values due to the 3 rows in the first matrix, weight.

While, we try to add the input at the first matrix, it tries to multiply the
values from the input list with the first values from the three lists from wieghts;
and thus it faces the error where it finds a pair of values for the first three
values from input, but the forth value does not have a pairing value from weights.
"""

#==============================================================================
# USING OBJECT ORIENTED PROGRAMMING
#==============================================================================

"""
The aim would be to
    1. take a batch of input, and
    2. program multiplelayers of neurons
"""

# 32 batch size is standard

input_set1 = [[1.0, 2.0, 3.0, 0.5],
              [2.0, 5.0,-1.0, 2.0],
              [-1.5, 2.7, 3.3, -0.8]] # 3 batches of inputs, each row represents a batch

weight_layer1 = [[0.1, -0.2, 1.2, -1.5],
                 [-0.5, 1.0, 0.4, 0.5],
                 [2.0, 1.7, 0.6, -1.8]] # 3 neurons with connections to 4 input neuron

# there is no change here as the shape of this list depends on # of neurons, not batches of inputs

bias_layer1 = [2, 3, 0.5] # biases for 3 neurons

# both bias and weights are specific to the respective neurons.

print(np.shape(input_set1)) # output: (3, 4)
print(np.shape(weight_layer1)) # output: (3, 4)

output_layer1 = np.dot(input_set1, weight_layer1) + bias_layer1
    # ValueError: shapes (3,4) and (3,4) not aligned: 4 (dim 1) != 3 (dim 0)
output_layer1 = np.dot(weight_layer1, input_set1) + bias_layer1
    # ValueError: shapes (3,4) and (3,4) not aligned: 4 (dim 1) != 3 (dim 0)
    
# both options do not work at the rows and columns do not match

output_layer1 = np.dot(input_set1, np.transpose(weight_layer1)) + bias_layer1
    # option 1; transposed the list of list using transpose function of np
print(output_layer1)

""" output:
[[ 4.55  5.95  6.8 ]
 [-3.    7.6   8.8 ]
 [ 6.47  7.37  5.51]]
"""

output_layer1 = np.dot(input_set1, np.array(weight_layer1).T) + bias_layer1
    # option 2; converted weights to an np array and transposed it
print(output_layer1)

""" output:
[[ 4.55  5.95  6.8 ]
 [-3.    7.6   8.8 ]
 [ 6.47  7.37  5.51]]
"""

weight_layer2 = [[0.1, -0.14,0.5],
                 [-0.5,0.12, -0.33],
                 [-0.44, 0.73, -0.13]] # 3 neurons with connections to 3 layer1 neurons

bias_layer2 = [-1, 2, -0.5] # biases for 3 neurons

# running the inputs through the 2 layers of neurons

output_layer1 = np.dot(input_set1, np.array(weight_layer1).T) + bias_layer1
output_layer2 = np.dot(output_layer1, np.array(weight_layer2).T) + bias_layer2

"""
we transpose as we need to multiply row[0] of output_layer1 with row[0] of
weight layer 2, and if we did not transpose then we would mutliple row[0] of output
with column [0] of weights.
"""

print(output_layer2)

""" output:
[[ 2.022  -1.805   0.9575]
 [ 2.036   1.508   5.224 ]
 [ 1.3702 -2.1689  1.317 ]]
"""

#==============================================================================
ipython = get_ipython() # to later clear out the kernel
if ipython is not None:
    ipython.magic('reset -f')  # Resets the current namespace
print("This is a clean console")

# introducing the concept of objects for optimization

import numpy as np
# import np.random as genran
np.random.seed(0) # to ensure we obtain the same value(s) every time we run the code

X = [[1.0, 2.0, 3.0, 0.5],
     [2.0, 5.0,-1.0, 2.0],
     [-1.5, 2.7, 3.3, -0.8]] # standard variable to denote "input"

"""
    When you are initiatizing the simulation model, either you can load a
    pre-defined model that contains the weights and biases.

    OR, you could create a new neural network model.
    In this case, you will initizialize new weights and biases' values.
    It is standard to keep the weight values between a small range like
        0 to 1 OR -1 to 1
    One could normalize or scale an existing model to get all the weight values
    between this range.
    
    When weights are 0s, it is prefered to keep the biases non-0 value
"""

class Layer_Dense: # defining the class of a layer of neurons
    def __init__(self, n_inputs, n_neurons): # will create a layer of neurons
        self.weights = 0.10*np.random.randn(n_inputs, n_neurons)
        self.biases = np.zeros((1, n_neurons)) # first param is the shape, hence the tuple "()"
    
    # using N of input to define the shape will eliminate the need to use transpose
    
    def forward(self, inputs): # method to push the input values from previous layer to next layer
        self.output = np.dot(inputs, self.weights) + self.biases

"""print(0.10*np.random.randn(4,10))"""

layer1 = Layer_Dense(4,10)
layer2 = Layer_Dense(10,5) # input has to match previous neurons
layer3 = Layer_Dense(5,2) # input has to match previous neurons

layer1.forward(X) # putting input batches into layer 1 of neurons
print(layer1.output)

""" output:
[[-0.55293562  0.54586413  0.46502292 -0.07326501  0.93905923 -0.45948518
   0.46906675 -0.05220354  0.49275908  0.2959332 ]
 [ 0.71112072  0.81743884  0.31226569  0.18707336  0.29888532  0.14808795
   1.1785395   0.12634378 -0.0948532  -0.5523246 ]
 [-1.08059852  0.5180714   0.41495629 -0.38973246  0.61656137 -0.25576484
   0.17756514 -0.19065021  0.63681439  0.21687699]]
"""

layer2.forward(layer1.output) # putting output of layer 1 neurons to layer 2
print(layer2.output)
""" output:
[[-0.01893487  0.15864686  0.08728175 -0.53031819  0.1553084 ]
 [-0.17276157 -0.29440293  0.04696612 -0.18272152 -0.07945568]
 [-0.00150442  0.22335463  0.18672203 -0.53895065  0.19034613]]
"""

layer3.forward(layer2.output) # putting output of layer 2 neurons to layer 3
print(layer3.output)
""" output:
[[ 0.00859227 -0.06911825]
 [ 0.00130714 -0.08237892]
 [ 0.01321654 -0.05377484]]
"""

#==============================================================================
# ACTIVATION FUNCTION AND ITS TYPES
#==============================================================================

"""

0. Intro

    Activation fuction comes after the inputs go through the weights and the
    biase is added to the output of the neuron.
    Every neuron in the hidden layers and the output layer will have an
    activation function.
    It is like a step function where the calculated output gets put in a
    function and the function output a new value which is then sent through the
    network as the new input.
    
1. Types
    1.1 Step Function - the activation function always outputs a 0 or 1 value.
    1.2 Sigmoid - more granular output than the step function: range 0 to 1.
    1.3 Rectifiec Linear Unit [ReLU] - closer to step, all values < 0 = 0,
        all values > 0 = value
    
    There is something called as optimizers.
    
    How do neural networks reach to the conclusions.
"""

# A version of the ReLU function:

relu_input = [1, 4, 3, -1, 0, 9, -4, -2, -5]
relu_output_trial1 = []

def relu_basic(input):
    output = []
    for i in input:
        if i > 0:
            output.append(i)
        elif i <= 0:
            output.append(0)
    return output

relu_basic(relu_input) # output: [1, 4, 3, 0, 0, 9, 0, 0, 0]

def relu_easy(input):
    output = []
    for i in input:
        output.append(max(0, i))
    return output

relu_easy(relu_input) # output: [1, 4, 3, 0, 0, 9, 0, 0, 0]

#==============================================================================

# pip install nnfs - installed the library package "Neural Networks from Scratch

import numpy as np
import nnfs

nnfs.init() # defined data type as float, and set randon_seed to 0

from nnfs.datasets import spiral_data

# spiral data function explanation below:
"""
def spiral_data(points, classes):
    X = np.zeros((points*classes, 2))
    y = np.zeros(points*classes, dtype='uint8')
    for class_number in range(classes):
        ix = range(points*class_number, points*(class_number+1))
        r = np.linspace(0.0, 1, points)  # radius
        t = np.linspace(class_number*4, (class_number+1)*4, points) + np.random.randn(points)*0.2
        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]
        y[ix] = class_number
    return X, y
"""
# takes in number of data points per class, number of classes, and outputs np array

X, y = spiral_data(100, 3) # no. of features or data points per row = 2

class Layer_Dense:
    def __init__(self, n_inputs, n_neurons):
        self.weights = 0.10*np.random.randn(n_inputs, n_neurons)
        self.biases = np.zeros((1, n_neurons))
    
    def forward(self, inputs):
        self.output = np.dot(inputs, self.weights) + self.biases

class Activation_ReLU:
    def forward(self, inputs):
        self.output = np.maximum(0, inputs)

layer1= Layer_Dense(2, 16)
active1 = Activation_ReLU()

layer1.forward(X)
print(layer1.output)
active1.forward(layer1.output)
print(active1.output)

""" exploring this aspect of activation """

layer2 = Layer_Dense(16, 8)
layer2.forward(active1.output)
active2 = Activation_ReLU()
active2.forward(layer2.output)

layer3 = Layer_Dense(8, 2)
layer3.forward(active2.output)
active3 = Activation_ReLU()
active3.forward(layer3.output)

#==============================================================================
# SOFTMAX ACTIVATION FUNCTION
#==============================================================================

layer_outputs = [4.8, 1.21, 2.385]

"""
    Introducing the Exponential function in order to standardize the values from
    the last layer of neurons into the output later of neurons.
        1. Helps us build a relation between the various neurons in the output layer
        2. Is an important change in training neural networks
        3. Helps us deal with negative values when establishing the relations
"""
# we are going to exponentiate the outputs using Euler's number

"""
E = 2.71828182846 # hard-coding the euler's number
"""

import math
E = math.e # importing euler's number using 'math' library

exp_values = [] # exponential values

for output in layer_outputs:
    exp_values.append(E**output)
    
print(exp_values) # output: [121.51041751873483, 3.353484652549023, 10.859062664920513]

"""
We now normalize the exponentiated values to understand the relation between the
values by dividing each value by the sum of the values.
"""

norm_base = sum(exp_values) # normalization base value
norm_values = []

for value in exp_values:
    norm_values.append(value/norm_base)

print(norm_values) # output: [0.8952826639572619, 0.024708306782099374, 0.0800090292606387]
print(sum(norm_values)) # output: 0.9999999999999999

# now converting this code using Numpy and cleaning it ========================

import math
import numpy as np
import nnfs

nnfs.init()

layer_outputs = [4.8, 1.21, 2.385]

exp_values = np.exp(layer_outputs)
print(exp_values) # output: [121.51041752   3.35348465  10.85906266]

norm_values = exp_values/np.sum(exp_values)
print(norm_values) # output: [0.89528266 0.02470831 0.08000903]
print(sum(norm_values)) # output: 0.9999999999999999

# This exponentiation and normalization is called SOFTMAX =====================

# using this for a batch of inputs

import math
import numpy as np
import nnfs

nnfs.init()

layer_outputs = [[4.8, 1.21, 2.385],
                 [8.9, -1.81,0.2],
                 [1.41, 1.051, 0.026]]

exp_values = np.exp(layer_outputs)
print(exp_values)
    # output:
"""
[[1.21510418e+02 3.35348465e+00 1.08590627e+01]
 [7.33197354e+03 1.63654137e-01 1.22140276e+00]
 [4.09595540e+00 2.86051020e+00 1.02634095e+00]]
"""
norm_base = np.sum(exp_values, axis=1, keepdims=True)
    # axis ensures that the np.sum applies to the axis 1, ie each row
    # keepdims allows function to match dimensions of output with input
    
print(norm_base)
    # output:
"""
[[ 135.72296484]
 [7333.35859605]
 [   7.98280655]]
"""
norm_values = exp_values / norm_base
print(norm_values)
    # output:
"""
[[8.95282664e-01 2.47083068e-02 8.00090293e-02]
 [9.99811129e-01 2.23163963e-05 1.66554348e-04]
 [5.13097164e-01 3.58333899e-01 1.28568936e-01]]
"""
print(np.sum(norm_values, axis=1, keepdims=True))
    # output:
"""
[[1.]
 [1.]
 [1.]]
"""

# addressing issues of overflow with exponentiation ===========================
"""
We subtract the max value in each batch of values from all the values in the batch
to reduce the value fed into the exponentiation function and thus the output gets
restricted between 0 and 1.
"""

# =============================================================================
# including subtracting max value and softmax activation class
# =============================================================================

# importing all the important libraries and functions
import math
import numpy as np
import nnfs
from nnfs.datasets import spiral_data

# setting the random.seed and data type
nnfs.init()

# defining the important object classes, attributes, and methods
class Layer_Dense:
    def __init__(self, n_inputs, n_neurons):
        self.weights = 0.10*np.random.randn(n_inputs, n_neurons)
        self.biases = np.zeros((1, n_neurons))
    
    def forward(self, inputs):
        self.output = np.dot(inputs, self.weights) + self.biases

class Activation_ReLU:
    def forward(self, inputs):
        self.output = np.maximum(0, inputs)
        
class Activation_Softmax:
    def forward(self, inputs):
        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))
        prob_values = exp_values/np.sum(exp_values, axis=1, keepdims=True)
        self.output = prob_values

# creating a custom dataset
X, y = spiral_data(samples=100, classes=3)

# creating the neural network's neurons
dense1 = Layer_Dense(2, 3)
activation1 = Activation_ReLU()
dense2 = Layer_Dense(3, 3)
activation2 = Activation_Softmax()

# passing data through the neural network
dense1.forward(X) #got values for layer 1 neurons from input, weight product plus biases
activation1.forward(dense1.output) #got activation values within range 0 and value for layer 1 neurons
dense2.forward(activation1.output) #got values for layer 2 neurons from activation inputs, weights product plus neuron biases
activation2.forward(dense2.output) #got output values as probabilities for output neurons

# looking at the results
print(activation2.output[:10])
    # output:
"""
[[0.33333334 0.33333334 0.33333334]
 [0.33331734 0.3333183  0.33336434]
 [0.3332888  0.33329153 0.33341965]
 [0.33325943 0.33326396 0.33347666]
 [0.33323312 0.33323926 0.33352762]
 [0.33328417 0.33328718 0.33342862]
 [0.33318216 0.33319145 0.33362636]
 [0.33318278 0.33319202 0.33362517]
 [0.33314922 0.33316055 0.3336902 ]
 [0.3331059  0.33311984 0.3337743 ]]
"""

# =============================================================================
# UNDERSTANDING AND CALCULATING LOSS 
# =============================================================================

""" Categorical Cross Entropy
Taking the negative sum of the target value mutliplied by the log of the predicted value
for each of the values

Key concepts
1. Logarithms
2. One-hot Coding
"""

# understanding logarithm
"""
ln(b) = x is solving for the value of x, where
e**x = b

simply put, when a**x = c, log of base a for value b is equal to the power of x
there are conventions:
    1. log(b) means 10**x = b
    2. ln(b) means e**x = b
"""

import numpy as np
import math

b = 5.2

print(np.log(b)) # output: 1.6486586255873816
print(math.e**np.log(b)) # output: 5.199999999999999

"""
Classes: 3
Label: 0 # the correct classification
One-Hotcoding: [1, 0, 0] # the [i]th index in one-hot is 1 when i = label
Prediction: [0.7, 0.1, 0.2]

Loss = L = -(1*log(0.7) + 0*log(0.1) + 0*log(0.2))
which simplifies to:
    L = -(1*log(0.7))
which reads as the negative product of log of predicted value for target class.
"""

# calculating loss with an example

import math

softmax_output = [0.7, 0.1, 0.2]
# target_class = 0
one_hot_coding = target_output = [1, 0, 0]

loss = -(math.log(softmax_output[0])*target_output[0] +
         math.log(softmax_output[1])*target_output[1] +
         math.log(softmax_output[2])*target_output[2])

print(loss) # output: 0.35667494393873245

loss_2 = -(math.log(softmax_output[0]))
print(loss_2) # output: 0.35667494393873245

""" The values are the same here"""

loss_3 = -math.log(0.75)
loss_4 = -math.log(0.25)

print(loss_3) # output: 0.2876820724517809
print(loss_4) # output: 1.3862943611198906

"""
Here, when the softmax output, ie, the prediction is higher, the loss value
is less; while the loss is much higher for a lower prediction value.

This can help us understand whether the predicted value for the target class is
an accurate prediction of the target class. As the loss reduced, the accurary 
increases.
"""

# APPLYING LOSS TO A BATCH ====================================================

softmax_output = [[0.7, 0.1, 0.2],
                  [0.1, 0.5, 0.4],
                  [0.02, 0.9, 0.08]]

class_targets = [0, 1, 1] # sparse coding
    # each value represents a class: {0:dog, 1:cat, 2:human}

"""
desired values are:
    softmax_output[0][0], softmax_output[1][1], and softmax_output[2][1]
We can acheive this using various methods.
"""

# method 1 - zipping

for targ_idx, outputs in zip(class_targets, softmax_output):
    print(outputs[targ_idx])
    # output
"""
0.7
0.5
0.9
"""    

# method 2 - Numpy array

import numpy as np

softmax_output = np.array([[0.7, 0.1, 0.2],
                          [0.1, 0.5, 0.4],
                          [0.02, 0.9, 0.08]])

class_targets = [0, 1, 1] # will be used as indices

target_values = softmax_output[[0,1,2], class_targets]

print(target_values) # output: [0.7 0.5 0.9]

"""
the values "[0,1,2]" refer to each individial row in the numpy
array that we want to input into the function. Instead of writing each row's 
index every time, as we might have as many as 100 batches, we can just take
the length of the array, and then input that into the range function, that will
output all the raw values as indices for us.

thus we replace with:
    range(len(softmax_output))
"""

print(softmax_output[range(len(softmax_output)), class_targets])
    # output: [0.7 0.5 0.9]
    
"""
And then to calculate the loss, we just add the negative and log function to this
to obtain the loss function for the predicted value per batch of the target value.
"""

print(-np.log(softmax_output[
    range(len(softmax_output)), class_targets]))
        # output: [0.35667494 0.69314718 0.10536052]
"""
What I understand is that the second batch that predicted the category for cat
is the most inaccurate, and conversely the third batch accurately classified the
inputs as a cat.
"""

# loss for the batch

neg_log = -np.log(softmax_output[range(len(softmax_output)), class_targets])
avg_loss = np.mean(neg_log)

print(avg_loss) # output: 0.38506088005216804

"""
The problem comes when we input the value zero into the log function, as the
-log of zero is infinite.

We could solve this by clipping the values by an insignificant amount.
"""

# understanding clipping

import numpy as np

print(np.log(1e-7)) # output: -16.11809565095832
print(np.log(1-1e-7)) # output: -1.0000000494736474e-07

# we then clip all the predicted values
    # y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)

# APPLIYING LOSS TO THE MODEL =================================================

import math
import numpy as np
import nnfs
from nnfs.datasets import spiral_data

nnfs.init()

class Layer_Dense:
    def __init__(self, n_inputs, n_neurons):
        self.weights = 0.10*np.random.randn(n_inputs, n_neurons)
        self.biases = np.zeros((1, n_neurons))
    
    def forward(self, inputs):
        self.output = np.dot(inputs, self.weights) + self.biases

class Activation_ReLU:
    def forward(self, inputs):
        self.output = np.maximum(0, inputs)
        
class Activation_Softmax:
    def forward(self, inputs):
        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))
        prob_values = exp_values/np.sum(exp_values, axis=1, keepdims=True)
        self.output = prob_values
        
class Loss: # defining a parent class here
    def calculate(self, output, y):
        sample_losses = self.forward(output, y) # this forward function will depend on the kind of loss function.
        data_loss = np.mean(sample_losses)
        return data_loss

class Loss_CategoricalCrossentropy(Loss):
    def forward(self, y_pred, y_true):
        samples = len(y_pred)
        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)
        
        if len(y_true.shape) == 1: # scalar values have been passed for the target classes
            correct_confidence = y_pred_clipped[range(samples), y_true] # same as softmax_output[range(len(softmax_output)), class_targets]
        elif len(y_true.shape) == 2: # one-hot encoded vectors passed
            correct_confidence = np.sum(y_pred_clipped*y_true, axis=1)
        
        negative_log_likelihoods = -np.log(correct_confidence)
        return negative_log_likelihoods

# here, output = y_pred, y = y_true, negative_log_likelihoods = sample_losses

X, y = spiral_data(samples=100, classes=3)

dense1 = Layer_Dense(2, 3)
activation1 = Activation_ReLU()
dense2 = Layer_Dense(3, 3)
activation2 = Activation_Softmax()

dense1.forward(X) 
activation1.forward(dense1.output)
dense2.forward(activation1.output)
activation2.forward(dense2.output)

print(activation2.output[:10])

loss_function = Loss_CategoricalCrossentropy()
loss = loss_function.calculate(activation2.output, y) # obtained the mean loss value for the predictions

print("Loss:", loss) # output: Loss: 1.098445

# CHECKING FOR ACCURACY =======================================================

"""
From the softmax outputs and the class targets, we can also determine how accurate
the predictions were. The principle is that the value at the index of the target
class each batch should be the highest value. If that is the case, then the 
model is accurate. Across batches, the average of the "and" statement for the 
target class and the index of the max value willprovide us with the accuracy.
"""

import numpy as np

softmax_output = np.array([[0.7, 0.1, 0.2],
                          [0.5, 0.4, 0.1],
                          [0.02, 0.9, 0.08]])

class_targets = [0, 1, 2]

predictions = np.argmax(softmax_output, axis=1)
print(predictions == class_targets) # output: [ True  True  True]

accuracy = np.mean(predictions == class_targets)
print(accuracy) # output: 0.3333333333333333

"""
While training a neural network, however, the loss metric is more important.

This loss can be reduced by modifying the weights and biases.
"""
